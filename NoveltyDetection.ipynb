{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from data_parser import DataParser\n",
    "from pull_data import Pull\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "from statistics import mean\n",
    "\n",
    "#from scikit_IsolatedForest import IsolatedForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "#from scikit_LOFNovelty import LOFNovelty\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "#from scikit_OneClassSVM import OCSVM\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKEA_APP = \"dev-annotated-datasets/ikea-app\"\n",
    "IKEA_HOMEKIT = \"dev-annotated-datasets/ikea-homekit\"\n",
    "IP_CAM = \"dev-annotated-datasets/ipcam\"\n",
    "NORMAL_USER = \"dev-annotated-datasets/normal-user\"\n",
    "VOICE_ASSISTANT = \"dev-annotated-datasets/voice-assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self,label):\n",
    "        self.label = label\n",
    "        self.accuracy = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1 = []\n",
    "        self.cnt = 0\n",
    "    def update(self,y,pred):\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "        except Exception as e:\n",
    "            # TN in all cases\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = confusion_matrix(y, pred).ravel()[0]\n",
    "        \n",
    "        total = tp+tn+fp+fn\n",
    "        accuracy = (tp+tn)/total\n",
    "        if self.label == \"Valid\":\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1 = 2*(precision*recall)/(precision+recall)\n",
    "        else:\n",
    "            try:\n",
    "                precision = tn/(tn+fn) # Negative precision\n",
    "                recall = tn/(tn+fp) # Negative recall\n",
    "                f1 = 2*(precision*recall)/(precision+recall) # Negative f1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                precision = 0\n",
    "                f1 = 0\n",
    "                recall = 0\n",
    "        \n",
    "        self.accuracy.append(accuracy)\n",
    "        self.precision.append(precision)\n",
    "        self.recall.append(recall)\n",
    "        self.f1.append(f1)\n",
    "        self.cnt += 1\n",
    "        \n",
    "    def print(self):\n",
    "        table = PrettyTable()\n",
    "        table.field_names = [self.label+\" Data\",\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"]\n",
    "        for i in range(len(self.accuracy)):\n",
    "            table.add_row([i,round(self.accuracy[i],3),round(self.precision[i],3),round(self.recall[i],3),round(self.f1[i],3)])\n",
    "        \n",
    "        table.add_row([\"Avg\",round(mean(self.accuracy),3),round(mean(self.precision),3),round(mean(self.recall),3),round(mean(self.f1),3)])\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y, pred, thr_pred=0.5, label=\"\"):\n",
    "    #mse = metrics.mean_squared_error(y, pred) # MSE of (y - pred) is the same as Brier score\n",
    "#    brier = metrics.brier_score_loss(y, pred)\n",
    "    #logloss = metrics.log_loss(y, pred)\n",
    "    print(\"### Metric\",label,\"###\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    except Exception as e:\n",
    "        # TN in all cases\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tp = confusion_matrix(y, pred).ravel()[0]\n",
    "    #print(tn, fp, fn, tp)\n",
    "    \n",
    "    total = tp+tn+fp+fn\n",
    "    acc = (tp+tn)/total\n",
    "    if label == \"Valid\":\n",
    "        prec = tp/(tp+fp)\n",
    "        rec = tp/(tp+fn)\n",
    "        f1 = 2*(prec*rec)/(prec+rec)\n",
    "    \n",
    "        print(\"TP: {:7d} {:6.2f}%\".format(tp, tp*100/total))\n",
    "        print(\"FN: {:7d} {:6.2f}%\".format(fn, fn*100/total))\n",
    "        print(\"FP: {:7d} {:6.2f}%\".format(fp, fp*100/total))\n",
    "        print(\"TN: {:7d} {:6.2f}%\".format(tn, tn*100/total))\n",
    "        print(\"Accuracy:   {:6.2f}%\".format(acc*100))\n",
    "        print(\"Precision:  {:6.4f}\".format(prec))\n",
    "        print(\"Recall:     {:6.4f}\".format(rec))\n",
    "        print(\"F1 score:   {:6.4f}\".format(f1))\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            prec_n = tn/(tn+fn)\n",
    "            rec_n = tn/(tn+fp)\n",
    "            f1_n = 2*(prec_n*rec_n)/(prec_n+rec_n)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            prec_n = 0\n",
    "            f1_n = 0\n",
    "            rec_n = 0\n",
    "        \n",
    "        print(\"TP: {:7d} {:6.2f}%\".format(tp, tp*100/total))\n",
    "        print(\"FN: {:7d} {:6.2f}%\".format(fn, fn*100/total))\n",
    "        print(\"FP: {:7d} {:6.2f}%\".format(fp, fp*100/total))\n",
    "        print(\"TN: {:7d} {:6.2f}%\".format(tn, tn*100/total))\n",
    "        print(\"Accuracy:   {:6.2f}%\".format(acc*100))\n",
    "        print(\"Precision Anomaly:  {:6.4f}\".format(prec_n))\n",
    "        print(\"Recall Anomaly:     {:6.4f}\".format(rec_n))\n",
    "        print(\"F1 score Anomaly:   {:6.4f}\".format(f1_n))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(models):\n",
    "    for key, model in models.items():\n",
    "        print(\"### Model Name:\",key,\" ###\")\n",
    "        m_valid = Metrics(label=\"Valid\")\n",
    "        m_anomaly = Metrics(label=\"Anomaly\")\n",
    "        kf = KFold(5, True)\n",
    "        t_data = np.array(t.data)\n",
    "        a_data = np.array(a.data)\n",
    "        iteration_cnt = 0\n",
    "        for train_index, test_index in kf.split(t_data):\n",
    "            iteration_cnt += 1\n",
    "            #Train\n",
    "            model.fit(t_data[train_index])\n",
    "            #Evaluate \n",
    "            y_pred_valid = model.predict(t_data[test_index])\n",
    "            y_pred_outliers = model.predict(a.data)\n",
    "            # Add results to the metrics object\n",
    "            m_valid.update([1]*len(y_pred_valid),y_pred_valid)\n",
    "            m_anomaly.update([-1]*len(y_pred_outliers),y_pred_outliers)\n",
    "            #print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"Valid\")\n",
    "            #print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"Anomaly\")\n",
    "        m_valid.print()\n",
    "        m_anomaly.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: 388  Anomaly: 16  Valid: 0\n",
      "Number of features: 110\n"
     ]
    }
   ],
   "source": [
    "t = Pull(IKEA_APP+\"/train/\",1)\n",
    "a = Pull(IKEA_APP+\"/anomaly/\",1)\n",
    "v = Pull(IKEA_APP+\"/valid/\",1)\n",
    "print(\"Valid:\",len(t.data),\" Anomaly:\",len(a.data),\" Valid:\",len(v.data))\n",
    "print(\"Number of features:\",t.features_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {}\n",
    "MODELS[\"IsolatedForest\"] = {}\n",
    "MODELS[\"LOF\"] = {}\n",
    "MODELS[\"OneClassSVM\"] = {}\n",
    "rng = np.random.RandomState(12345)\n",
    "MODELS[\"IsolatedForest\"][\"IF1\"] = IsolationForest(n_estimators = 100, max_samples=\"auto\",max_features=1,bootstrap=False ,random_state=rng, behaviour='new', contamination='auto')\n",
    "MODELS[\"LOF\"][\"LOF1\"] = LocalOutlierFactor(n_neighbors = 20, metric = \"chebyshev\", novelty=True, contamination='auto')\n",
    "MODELS[\"OneClassSVM\"][\"OSVM1\"] = OneClassSVM(kernel='sigmoid',gamma=\"auto\",coef0=0.0, nu=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Name: IF1  ###\n",
      "+------------+----------+-----------+--------+----------+\n",
      "| Valid Data | Accuracy | Precision | Recall | F1 score |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "|     0      |  0.974   |    1.0    | 0.974  |  0.987   |\n",
      "|     1      |  0.974   |    1.0    | 0.974  |  0.987   |\n",
      "|     2      |  0.923   |    1.0    | 0.923  |   0.96   |\n",
      "|     3      |  0.961   |    1.0    | 0.961  |   0.98   |\n",
      "|     4      |   1.0    |    1.0    |  1.0   |   1.0    |\n",
      "|    Avg     |  0.967   |    1.0    | 0.967  |  0.983   |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Anomaly Data | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|      0       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|      1       |  0.312   |    1.0    | 0.312  |  0.476   |\n",
      "|      2       |  0.312   |    1.0    | 0.312  |  0.476   |\n",
      "|      3       |  0.062   |    1.0    | 0.062  |  0.118   |\n",
      "|      4       |  0.188   |    1.0    | 0.188  |  0.316   |\n",
      "|     Avg      |  0.325   |    1.0    | 0.325  |  0.449   |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "### Model Name: LOF1  ###\n",
      "+------------+----------+-----------+--------+----------+\n",
      "| Valid Data | Accuracy | Precision | Recall | F1 score |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "|     0      |  0.718   |    1.0    | 0.718  |  0.836   |\n",
      "|     1      |  0.744   |    1.0    | 0.744  |  0.853   |\n",
      "|     2      |  0.692   |    1.0    | 0.692  |  0.818   |\n",
      "|     3      |  0.818   |    1.0    | 0.818  |   0.9    |\n",
      "|     4      |   0.74   |    1.0    |  0.74  |  0.851   |\n",
      "|    Avg     |  0.742   |    1.0    | 0.742  |  0.852   |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Anomaly Data | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|      0       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|      1       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|      2       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|      3       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|      4       |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "|     Avg      |   0.75   |    1.0    |  0.75  |  0.857   |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "### Model Name: OSVM1  ###\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "+------------+----------+-----------+--------+----------+\n",
      "| Valid Data | Accuracy | Precision | Recall | F1 score |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "|     0      |   0.0    |    nan    |  0.0   |   nan    |\n",
      "|     1      |   0.0    |    nan    |  0.0   |   nan    |\n",
      "|     2      |   0.0    |    nan    |  0.0   |   nan    |\n",
      "|     3      |   0.0    |    nan    |  0.0   |   nan    |\n",
      "|     4      |   0.0    |    nan    |  0.0   |   nan    |\n",
      "|    Avg     |   0.0    |    nan    |  0.0   |   nan    |\n",
      "+------------+----------+-----------+--------+----------+\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Anomaly Data | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|      0       |   1.0    |     0     |   0    |    0     |\n",
      "|      1       |   1.0    |     0     |   0    |    0     |\n",
      "|      2       |   1.0    |     0     |   0    |    0     |\n",
      "|      3       |   1.0    |     0     |   0    |    0     |\n",
      "|      4       |   1.0    |     0     |   0    |    0     |\n",
      "|     Avg      |   1.0    |     0     |   0    |    0     |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "runModel(MODELS[\"IsolatedForest\"])\n",
    "runModel(MODELS[\"LOF\"])\n",
    "runModel(MODELS[\"OneClassSVM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Iteration: 1 =====\n",
      "### Metric Valid ###\n",
      "TP:     125  96.15%\n",
      "FN:       5   3.85%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    96.15%\n",
      "Precision:  1.0000\n",
      "Recall:     0.9615\n",
      "F1 score:   0.9804\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:      11  68.75%\n",
      "TN:       5  31.25%\n",
      "Accuracy:    31.25%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.3125\n",
      "F1 score Anomaly:   0.4762\n",
      "===== Iteration: 2 =====\n",
      "### Metric Valid ###\n",
      "TP:     110  85.27%\n",
      "FN:      19  14.73%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    85.27%\n",
      "Precision:  1.0000\n",
      "Recall:     0.8527\n",
      "F1 score:   0.9205\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:      10  62.50%\n",
      "TN:       6  37.50%\n",
      "Accuracy:    37.50%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.3750\n",
      "F1 score Anomaly:   0.5455\n",
      "===== Iteration: 3 =====\n",
      "### Metric Valid ###\n",
      "TP:     125  96.90%\n",
      "FN:       4   3.10%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    96.90%\n",
      "Precision:  1.0000\n",
      "Recall:     0.9690\n",
      "F1 score:   0.9843\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:       9  56.25%\n",
      "TN:       7  43.75%\n",
      "Accuracy:    43.75%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.4375\n",
      "F1 score Anomaly:   0.6087\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "rng = np.random.RandomState(12345)\n",
    "clf = IsolationForest(n_estimators = 100, max_samples=\"auto\",max_features=1,bootstrap=False ,random_state=rng, behaviour='new', contamination='auto')\n",
    "\n",
    "#Train \n",
    "#clf.fit(t.data)\n",
    "\n",
    "#Evaluate \n",
    "#y_pred_valid = clf.predict(v.data)\n",
    "#y_pred_outliers = clf.predict(a.data)\n",
    "\n",
    "# Measurement\n",
    "#print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"valid\")\n",
    "#print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"anomaly\")\n",
    "# Detection quality score ( The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.) \n",
    "#score_v = clf.decision_function(v.data)\n",
    "#score_a = clf.decision_function(a.data)\n",
    "#print(score_v)\n",
    "#print(score_a)\n",
    "\n",
    "\n",
    "#m_valid = Metrics(label=\"Valid\")\n",
    "#m_anomaly = Metrics(label=\"Anomaly\")\n",
    "kf = KFold(3, True)\n",
    "t_data = np.array(t.data)\n",
    "a_data = np.array(a.data)\n",
    "iteration_cnt = 0\n",
    "for train_index, test_index in kf.split(t_data):\n",
    "    iteration_cnt += 1\n",
    "    #Train\n",
    "    clf.fit(t_data[train_index])\n",
    "    #Evaluate \n",
    "    y_pred_valid = clf.predict(t_data[test_index])\n",
    "    y_pred_outliers = clf.predict(a.data)\n",
    "    print(\"===== Iteration:\",iteration_cnt,\"=====\")\n",
    "    print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"Valid\")\n",
    "    print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"Anomaly\")\n",
    "    \n",
    "    #m_valid.update([1]*len(y_pred_valid),y_pred_valid)\n",
    "    #m_anomaly.update([-1]*len(y_pred_outliers),y_pred_outliers)\n",
    "#m_valid.print()\n",
    "#m_anomaly.print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Iteration: 1 =====\n",
      "### Metric Valid ###\n",
      "TP:      95  73.08%\n",
      "FN:      35  26.92%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    73.08%\n",
      "Precision:  1.0000\n",
      "Recall:     0.7308\n",
      "F1 score:   0.8444\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:       4  25.00%\n",
      "TN:      12  75.00%\n",
      "Accuracy:    75.00%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.7500\n",
      "F1 score Anomaly:   0.8571\n",
      "===== Iteration: 2 =====\n",
      "### Metric Valid ###\n",
      "TP:      94  72.87%\n",
      "FN:      35  27.13%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    72.87%\n",
      "Precision:  1.0000\n",
      "Recall:     0.7287\n",
      "F1 score:   0.8430\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:       4  25.00%\n",
      "TN:      12  75.00%\n",
      "Accuracy:    75.00%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.7500\n",
      "F1 score Anomaly:   0.8571\n",
      "===== Iteration: 3 =====\n",
      "### Metric Valid ###\n",
      "TP:      81  62.79%\n",
      "FN:      48  37.21%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:    62.79%\n",
      "Precision:  1.0000\n",
      "Recall:     0.6279\n",
      "F1 score:   0.7714\n",
      "### Metric Anomaly ###\n",
      "TP:       0   0.00%\n",
      "FN:       0   0.00%\n",
      "FP:       3  18.75%\n",
      "TN:      13  81.25%\n",
      "Accuracy:    81.25%\n",
      "Precision Anomaly:  1.0000\n",
      "Recall Anomaly:     0.8125\n",
      "F1 score Anomaly:   0.8966\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "clf = LocalOutlierFactor(n_neighbors = 20, metric = \"chebyshev\", novelty=True, contamination='auto')\n",
    "\n",
    "#Train \n",
    "#clf.fit(t.data)\n",
    "\n",
    "#Evaluate \n",
    "#y_pred_valid = clf.predict(v.data)\n",
    "#y_pred_outliers = clf.predict(a.data)\n",
    "\n",
    "# Measurement\n",
    "#print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"valid\")\n",
    "#print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"anomaly\")\n",
    "# Detection quality score ( The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.) \n",
    "#score_v = clf.decision_function(v.data)\n",
    "#score_a = clf.decision_function(a.data)\n",
    "#print(score_v)\n",
    "#print(score_a)\n",
    "\n",
    "kf = KFold(3, True)\n",
    "t_data = np.array(t.data)\n",
    "a_data = np.array(a.data)\n",
    "iteration_cnt = 0\n",
    "for train_index, test_index in kf.split(t_data):\n",
    "    iteration_cnt += 1\n",
    "    #Train\n",
    "    clf.fit(t_data[train_index])\n",
    "    #Evaluate \n",
    "    y_pred_valid = clf.predict(t_data[test_index])\n",
    "    y_pred_outliers = clf.predict(a.data)\n",
    "    print(\"===== Iteration:\",iteration_cnt,\"=====\")\n",
    "    print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"Valid\")\n",
    "    print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"Anomaly\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Iteration: 1 =====\n",
      "### Metric Valid ###\n",
      "TP:       0   0.00%\n",
      "FN:     130 100.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:     0.00%\n",
      "Precision:     nan\n",
      "Recall:     0.0000\n",
      "F1 score:      nan\n",
      "### Metric Anomaly ###\n",
      "division by zero\n",
      "TP:      16 100.00%\n",
      "FN:       0   0.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:   100.00%\n",
      "Precision Anomaly:  0.0000\n",
      "Recall Anomaly:     0.0000\n",
      "F1 score Anomaly:   0.0000\n",
      "===== Iteration: 2 =====\n",
      "### Metric Valid ###\n",
      "TP:       0   0.00%\n",
      "FN:     129 100.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:     0.00%\n",
      "Precision:     nan\n",
      "Recall:     0.0000\n",
      "F1 score:      nan\n",
      "### Metric Anomaly ###\n",
      "division by zero\n",
      "TP:      16 100.00%\n",
      "FN:       0   0.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:   100.00%\n",
      "Precision Anomaly:  0.0000\n",
      "Recall Anomaly:     0.0000\n",
      "F1 score Anomaly:   0.0000\n",
      "===== Iteration: 3 =====\n",
      "### Metric Valid ###\n",
      "TP:       0   0.00%\n",
      "FN:     129 100.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:     0.00%\n",
      "Precision:     nan\n",
      "Recall:     0.0000\n",
      "F1 score:      nan\n",
      "### Metric Anomaly ###\n",
      "division by zero\n",
      "TP:      16 100.00%\n",
      "FN:       0   0.00%\n",
      "FP:       0   0.00%\n",
      "TN:       0   0.00%\n",
      "Accuracy:   100.00%\n",
      "Precision Anomaly:  0.0000\n",
      "Recall Anomaly:     0.0000\n",
      "F1 score Anomaly:   0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "#Create Model\n",
    "clf = OneClassSVM(kernel='sigmoid',gamma=\"auto\",coef0=0.0, nu=0.1)\n",
    "\n",
    "#Train \n",
    "#clf.fit(t.data)\n",
    "\n",
    "#Evaluate \n",
    "#y_pred_valid = clf.predict(v.data)\n",
    "#y_pred_outliers = clf.predict(a.data)\n",
    "\n",
    "# Measurement\n",
    "#print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"valid\")\n",
    "#print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"anomaly\")\n",
    "# Detection quality score ( Signed distance is positive for an inlier and negative for an outlier.) \n",
    "#score_v = clf.decision_function(v.data)\n",
    "#score_a = clf.decision_function(a.data)\n",
    "#print(score_v)\n",
    "#print(score_a)\n",
    "\n",
    "kf = KFold(3, True)\n",
    "t_data = np.array(t.data)\n",
    "a_data = np.array(a.data)\n",
    "iteration_cnt = 0\n",
    "for train_index, test_index in kf.split(t_data):\n",
    "    iteration_cnt += 1\n",
    "    #Train\n",
    "    clf.fit(t_data[train_index])\n",
    "    #Evaluate \n",
    "    y_pred_valid = clf.predict(t_data[test_index])\n",
    "    y_pred_outliers = clf.predict(a.data)\n",
    "    print(\"===== Iteration:\",iteration_cnt,\"=====\")\n",
    "    print_metrics([1]*len(y_pred_valid),y_pred_valid,label=\"Valid\")\n",
    "    print_metrics([-1]*len(y_pred_outliers),y_pred_outliers,label=\"Anomaly\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
